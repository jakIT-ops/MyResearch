{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Lesson Recap\n",
    "<a href='#the_destination'><b>Jump to the current lesson<b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "We have already about exploratory data analysis in the [Kaggle Challenge - Exploratory Data Analysis](https://www.educative.io/collection/page/4747639511842816/6260527028240384/5834129520197632) lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the CSV file (assume that the file is stored in the data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('./data/train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the columns in the pandas data frame using the `.columns` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "housing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the shape of the data using the `.shape` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of data\n",
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"shape\" of the dataset shows that it has 1460 rows/instances, with data from 80 attributes. \n",
    "Out of the 80 attributes, one is the target (SalePrice) that the model should predict. \n",
    "Hence, there are 79 attributes that may be used for feature selection/engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first five rows of the data set using the `.head()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the description about the data using the `.info()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.drop(\"Id\", axis=1)\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of numerical attributes by excluding the datatype of object using the `exclude=['object']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical attributes\n",
    "housing.select_dtypes(exclude=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the length after excluding the columns of object type using the `len` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(housing.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data summary with upto 2 decimals and call `transpose()` for a better view of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.select_dtypes(exclude=['object']).describe().round(decimals=2).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the categorical attributes by including datatype of type object using `include = ['object']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical attribute\n",
    "housing.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 43 categorical columns with the following characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the categorical attributes using `include=['object']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.select_dtypes(include=['object']).describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a detailed look at our target variable, SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics summary\n",
    "housing['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skew of target column\n",
    "It appears to be good practice to minimise the skew of the dataset. The reason often given is that skewed data adversely affects the prediction accuracy of regression models. \n",
    "Note: While important for linear regression, correcting skew is not necessary for Decisions Trees and Random Forests. \n",
    "\n",
    "(Not a needed step for target var, while we can scale the other numerical features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the distribution plot using the seaborn `.displot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distribution plot\n",
    "sns.displot(housing['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a log transformation means to simply take the log of the skewed variable to improve the fit by altering the scale and making the variable more “normally” distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the log to make the distribution more normal\n",
    "sns.displot(np.log(housing['SalePrice']))\n",
    "plt.title('Distribution of Log-transformed SalePrice')\n",
    "plt.xlabel('log(SalePrice)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the log-transformed variable is more “normally” distributed — we have managed to reduce the skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the distributions of all the numerical variables by calling the distplot() method in a for loop, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What about the distribution of all the other numerical variables?\n",
    "\n",
    "num_attributes = housing.select_dtypes(exclude='object').drop(['SalePrice'], axis=1).copy()\n",
    "\n",
    "print(len(num_attributes.columns))\n",
    "\n",
    "fig = plt.figure(figsize=(12,18))\n",
    "for i in range(len(num_attributes.columns)):\n",
    "    fig.add_subplot(9,4,i+1)\n",
    "    sns.distplot(num_attributes.iloc[:,i].dropna(), hist = False, rug = True)\n",
    "    plt.xlabel(num_attributes.columns[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see how skewed LotArea is — it is in dire need of some polishing before it can be used for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since box plots give us a good overview of our data. From the distribution of observations w.r.t. the upper and lower quartiles, we can spot outliers. Let’s see this in action with the boxplot() method and a for loop to plot all the attributes in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 15))\n",
    "\n",
    "for i in range(len(num_attributes.columns)):\n",
    "    fig.add_subplot(9, 4, i+1)\n",
    "    sns.boxplot(y=num_attributes.iloc[:,i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return value at the given quantile over requested axis.\n",
    "high_quant = housing.quantile(.999)\n",
    "high_quant['LotArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at the prices. The radius of each circle represents GrLivArea (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"OverallQual\", y=\"YearBuilt\",  s=housing[\"GrLivArea\"], label=\"GrLivArea\", alpha=0.3, \n",
    "             figsize=(10,7), c=\"SalePrice\", cmap=plt.get_cmap(\"jet\"), colorbar=True\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations Among Numerical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `corr()` method to very easily get the correlations and then visualize them using the `heatmap()` method – Python does feel like magic often, isn’t it?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of numerical attributes\n",
    "corr = housing.corr()\n",
    "\n",
    "# Using mask to get triangular correlation matrix\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, vmin = -1.0, vmax = 1.0, linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reference to the target SalePrice, the top correlated attributes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr['SalePrice'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these values, we can see that OverallQual and GrLivArea have the most impact on price, while attributes like PoolArea and MoSold are not related to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rough joint distribution plot for each pair of variables can be done by using `pairplot()`from sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['SalePrice', 'OverallQual', 'GrLivArea', 'YearBuilt']\n",
    "sns.pairplot(housing[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pairplots, we can clearly see how with an increase in GrLivArea the price increases as well. Do play around with other attributes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only important correlations and not auto-correlations\n",
    "threshold = 0.5\n",
    "important_corrs = (corr[abs(corr) > threshold][corr != 1.0]) \\\n",
    "    .unstack().dropna().to_dict()\n",
    "\n",
    "unique_important_corrs = pd.DataFrame(\n",
    "    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n",
    "    for key in important_corrs])), columns=['attribute pair', 'correlation'])\n",
    "\n",
    "# sorted by absolute value\n",
    "unique_important_corrs = unique_important_corrs.iloc[\n",
    "    abs(unique_important_corrs['correlation']).argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_important_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graphs for the highest corr var\n",
    "\n",
    "Get the bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(housing.OverallQual, housing.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot\n",
    "plt.figure(figsize=(18, 8))\n",
    "sns.boxplot(x=housing.OverallQual, y=housing.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the age of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'YearBuilt'\n",
    "data = pd.concat([housing['SalePrice'], housing[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(16, 8))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s just print again the names of the categorical columns first and then handpick some of the interesting ones for visual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = housing.select_dtypes(include='object').columns\n",
    "print(cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the impact of KitchQual on price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = housing['KitchenQual']\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sns.boxplot(y=housing.SalePrice, x=var)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that Ex seems to be the more expensive option while Fa brings the prices down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the style of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "sns.boxplot(y=housing.SalePrice, x=housing.HouseStyle)\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 2Story houses have the highest variability in prices and they also tend to be more expensive, while 1.5Unf are the cheapest option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the frequency for each of these types, we can use the countplot() method from sns like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of categories within HouseStyle attribute\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "sns.countplot(x='HouseStyle', data=housing)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get exact count for each category, we can get that very easily from the DataFrame directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"HouseStyle\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also curious to see if the style of the houses has changed over the years, so let’s plot the two variables against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(housing['YearBuilt'],housing['HouseStyle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing - Prepare the Data for Machine Learning Algorithms\n",
    "We have already learned about data preprocessing in the [Kaggle Challenge - Data Preprocessing\n",
    "](https://www.educative.io/collection/page/4747639511842816/6260527028240384/5263608478957568) lesson.\n",
    "\n",
    "\n",
    "<a href='#the_destination'><b>Jump to the current lesson<b></a>\n",
    " \n",
    "Get a sorted count of the missing values for all the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can assume that PoolQC to Bsmt attributes are missing for the houses that do not have these facilities (houses without pools, basements, garage etc.). Therefore, the missing values could be filled in with “None”. MasVnrType and MasVnrArea both have 8 missing values, likely houses without masonry veneer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal With Missing Values\n",
    "We are going to apply different approaches to fix our missing values, so that we can various approaches in action:\n",
    "\n",
    "* Replace values for categorical attributes with None.\n",
    "* Compute the median LotFrontage for all the houses in the same neighborhood, instead of the plain median for the entire column, and use that to impute on a neighborhood by neighborhood basis.\n",
    "* Replace missing values for most of the numerical columns with zero and one with the mode.\n",
    "* Drop one non-interesting column, Utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing Missing Values\n",
    "\n",
    "housing_processed = housing\n",
    "\n",
    "# Categorical columns:\n",
    "cat_cols_fill_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
    "                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n",
    "                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n",
    "                     'MasVnrType']\n",
    "\n",
    "# Replace missing values for categorical columns with None\n",
    "for cat in cat_cols_fill_none:\n",
    "    housing_processed[cat] = housing_processed[cat].fillna(\"None\")\n",
    "    \n",
    "# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "housing_processed['LotFrontage'] = housing_processed.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))    \n",
    "\n",
    "# GarageYrBlt, GarageArea and GarageCars these are numerical columns, replace with zero\n",
    "for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "    housing_processed[col] = housing_processed[col].fillna(int(0))\n",
    "    \n",
    "#MasVnrArea : replace with zero\n",
    "housing_processed['MasVnrArea'] = housing_processed['MasVnrArea'].fillna(int(0))\n",
    "\n",
    "#Use the mode value \n",
    "housing_processed['Electrical'] = housing_processed['Electrical'].fillna(housing_processed['Electrical']).mode()[0]\n",
    "\n",
    "#There is no need of Utilities so let's just drop this column\n",
    "housing_processed = housing_processed.drop(['Utilities'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count again to verify that we do not have any more missing values\n",
    "housing_processed.isnull().apply(sum).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal With Outliers\n",
    "Invoking the `quantile()` method on the DataFrame and then filtering based on the knowledge of the quantiles for each attribute, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = housing_processed.select_dtypes(exclude='object')\n",
    "\n",
    "high_quant = housing_processed.quantile(.999)\n",
    "\n",
    "for i in num_attributes.columns:\n",
    "    housing_processed = housing_processed.drop(housing_processed[i][housing_processed[i]>high_quant[i]].index)\n",
    "\n",
    "housing_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal With Categorical Attributes\n",
    "\n",
    "we can drop GarageArea because it is highly correlated with GarageCars and the reason for preferring GarageCars is because it is more correlated with price than area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove highly correlated features\n",
    "# Remove attributes that were identified for excluding when viewing scatter plots & corr values\n",
    "attributes_drop = ['MiscVal', 'MoSold', 'YrSold', 'BsmtFinSF2','BsmtHalfBath','MSSubClass',\n",
    "                   'GarageArea', 'GarageYrBlt', '3SsnPorch']\n",
    "\n",
    "housing_processed = housing_processed.drop(attributes_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Text and Categorical Data\n",
    "Let's convert all the categories from text to numbers.\n",
    "A common approach to deal with textual data is to create one binary attribute for each category of the feature: for example, for type of houses, we would have one attribute equal to 1 when the category is 1Story (and 0 otherwise), another attribute equal to 1 when the category is 2Story (and 0 otherwise), and so on. This is called one-hot encoding, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are also known as dummy attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Transforming Cat variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_processed_1hot = cat_encoder.fit_transform(housing_processed)\n",
    "housing_processed_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformation Pipelines\n",
    "\n",
    "We have already learned Data Preprocessing in [Kaggle Challenge - Data Transformation](https://www.educative.io/collection/page/4747639511842816/6260527028240384/5292909500825600) lesson.\n",
    "\n",
    "\n",
    "<a href='#the_destination'><b>Jump to the current lesson<b></a>\n",
    "\n",
    "Let's look at a simple example pipeline to impute and scale numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate features and target variable\n",
    "housing_X = housing_processed.drop(\"SalePrice\", axis=1)\n",
    "housing_y = housing_processed[\"SalePrice\"].copy()\n",
    "\n",
    "# Get the list of names for numerical and categorical attributes separately\n",
    "num_attributes = housing_X.select_dtypes(exclude='object')\n",
    "cat_attributes = housing_X.select_dtypes(include='object')\n",
    "\n",
    "num_attribs = list(num_attributes)\n",
    "cat_attribs = list(cat_attributes)\n",
    "\n",
    "# Numerical Pipeline to impute any missing values with the median and scale attributes\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have separated the SalePrice attribute into a separate variable, because for creating the machine learning model, we need to separate all the features, housing_X, from the target variable, housing_y.\n",
    "\n",
    "It is more convenient and clean to have a single transformer handle all columns, applying the appropriate transformations to each column. Scikit-Learn comes to the rescue again by providing the `ColumnTransformer` for the very purpose. Let’s use it to apply all the transformations to our data and create a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "\n",
    "# Description before applying transforms\n",
    "print(housing_y.describe())\n",
    "\n",
    "# Apply log-transform to SalePrice\n",
    "housing_y_prepared  = np.log(housing_y)\n",
    "\n",
    "# Run the transformation pipeline on all the other attributes\n",
    "housing_X_prepared = full_pipeline.fit_transform(housing_X)\n",
    "\n",
    "# Description before applying transforms\n",
    "print(housing_y_prepared.describe())\n",
    "\n",
    "housing_X_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Models \n",
    "We have already learned about data preprocessing in the [Kaggle Challenge - Machine Learning Models](https://www.educative.io/collection/page/4747639511842816/6260527028240384/5060702475649024) lesson.\n",
    "\n",
    "\n",
    "<a href='#the_destination'><b>Jump to the current lesson<b></a>\n",
    "\n",
    "Let's apply machine learning techniques to our transformed data.\n",
    "\n",
    "### Select and Train Models \n",
    "We'll split the transformed data set into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test formate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_X_prepared, housing_y_prepared, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **Linear Regression** Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost\n",
    "\n",
    "# Invert the log-transformed value\n",
    "def inv_y(transformed_y):\n",
    "    return np.exp(transformed_y)\n",
    "\n",
    "# Series to collect RMSE for the different algorithms: \"algorithm name + rmse\"\n",
    "rmse_compare = pd.Series()\n",
    "rmse_compare.index.name = 'Model'\n",
    "\n",
    "# Series to collect accuracy scores for the different algorithms: \"algorithm name + score\"\n",
    "scores_compare = pd.Series()\n",
    "scores_compare.index.name = 'Model'\n",
    "\n",
    "# Model 1: Linear Regression =================================================\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "linear_val_predictions = linear_model.predict(X_test)\n",
    "linear_val_rmse = mean_squared_error(inv_y(linear_val_predictions), inv_y(y_test))\n",
    "linear_val_rmse = np.sqrt(linear_val_rmse)\n",
    "rmse_compare['LinearRegression'] = linear_val_rmse\n",
    "\n",
    "lr_score = linear_model.score(X_test, y_test)*100\n",
    "scores_compare['LinearRegression'] = lr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **Decision Trees** Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Trees. Define the model. =============================\n",
    "dtree_model = DecisionTreeRegressor(random_state=5)\n",
    "dtree_model.fit(X_train, y_train)\n",
    "\n",
    "dtree_val_predictions = dtree_model.predict(X_test)\n",
    "dtree_val_rmse = mean_squared_error(inv_y(dtree_val_predictions), inv_y(y_test))\n",
    "dtree_val_rmse = np.sqrt(dtree_val_rmse)\n",
    "rmse_compare['DecisionTree'] = dtree_val_rmse\n",
    "\n",
    "dtree_score = dtree_model.score(X_test, y_test)*100\n",
    "scores_compare['DecisionTree'] = dtree_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **Random Forest** ALgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest. Define the model. =============================\n",
    "rf_model = RandomForestRegressor(random_state=5)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_val_predictions = rf_model.predict(X_test)\n",
    "rf_val_rmse = mean_squared_error(inv_y(rf_val_predictions), inv_y(y_test))\n",
    "rf_val_rmse = np.sqrt(rf_val_rmse)\n",
    "rmse_compare['RandomForest'] = rf_val_rmse\n",
    "\n",
    "rf_score = rf_model.score(X_test, y_test)*100\n",
    "scores_compare['RandomForest'] = rf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **Gradiest Boosting** Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Gradient Boosting Regression ==========================================\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, \n",
    "                                      max_depth=4, random_state=5)\n",
    "gbr_model.fit(X_train, y_train)\n",
    "\n",
    "gbr_val_predictions = gbr_model.predict(X_test)\n",
    "gbr_val_rmse = mean_squared_error(inv_y(gbr_val_predictions), inv_y(y_test))\n",
    "gbr_val_rmse = np.sqrt(gbr_val_rmse)\n",
    "rmse_compare['GradientBoosting'] = gbr_val_rmse\n",
    "\n",
    "gbr_score = gbr_model.score(X_test, y_test)*100\n",
    "scores_compare['GradientBoosting'] = gbr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **Root Mean Square Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE values for different algorithms:')\n",
    "rmse_compare.sort_values(ascending=True).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **Accuracy** by applying different Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy scores for different algorithms:')\n",
    "scores_compare.sort_values(ascending = False).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model, Linear Regression, seems to be performing the best, with predicted prices that are off by about 24K. This might or might not be an acceptable amount of deviation – depends on the desired level of accuracy or the metric we are trying to optimize based on our business objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "\n",
    "Let’s perform a K-fold cross-validation on our best model: the cross-validation function randomly splits the training set into K distinct subsets (folds), then it trains and evaluates the model K times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the K evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(linear_model, X_train, y_train,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "linear_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(linear_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf_model, X_train, y_train,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rf_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(rf_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we can notice that cross-validation gives us the mean and standard deviation for the scores as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the_destination'><h2>Current Lesson</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tune Your Model\n",
    "\n",
    "This is a model that has many input hyperparameters that can be tweaked for improving performance. For example, we could have a forest with 100 or 1000 trees, or we could use 10 or 50 features during random selection. What are the best values for these hyperparameters to pass as input to the model for training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Scikit-learn’s `GridSearchCV` to tell  which hyperparameters we would like to explore and which values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [10, 50, 100, 150], 'max_features': [10, 20, 30, 40, 50, 100, 150]},\n",
    "    {'bootstrap': [False], 'n_estimators': [10, 50, 100, 150], 'max_features': [10, 20, 30, 40, 50, 100, 150]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use best_params_ to visualize the best values for the passed hyperparameters, and best_estimator_ to get the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the optimal values for the hyperparameters (‘bootstrap’: False, ‘max_features’: 50, ‘n_estimators’: 150), let’s plug them in and see if our Random Forest model has improved compared to the vanilla Random Forest model that we trained earlier when we trained multiple models at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_final = RandomForestRegressor(bootstrap=False,max_features=50, n_estimators=150, random_state=5)\n",
    "\n",
    "rf_model_final.fit(X_train, y_train)\n",
    "rf_final_val_predictions = rf_model_final.predict(X_test)\n",
    "rf_final_val_rmse = mean_squared_error(inv_y(rf_final_val_predictions), inv_y(y_test))\n",
    "np.sqrt(rf_final_val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_final.score(X_test, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s display these importance scores next to their corresponding attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
