{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sum0Ncjcy-bU"
   },
   "source": [
    "# Deep Learning with PyTorch Step-by-Step: A Beginner's Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdlplLyKy-bW"
   },
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXwM8NeZy-bX"
   },
   "outputs": [],
   "source": [
    "from config_dl import *\n",
    "from plots.chapter2_1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vUynyDqy-bX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnVChfy0y-bY"
   },
   "source": [
    "# Going Classy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtZpVI5Dy-bY"
   },
   "source": [
    "## The Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44Pg8Sp_y-bY"
   },
   "outputs": [],
   "source": [
    "# A completely empty (and useless) class\n",
    "class StepByStep(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fiw4yAQuy-bY"
   },
   "source": [
    "## The Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQAPqatTy-bY"
   },
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybzZACIty-bZ"
   },
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2THul3py-bZ"
   },
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAvDudW8y-bZ"
   },
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # These attributes are defined here, but since they are\n",
    "        # not available at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader \n",
    "        # (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to create a SummaryWriter to \n",
    "        # interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(\n",
    "            folder, name, suffix\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJZT3k14y-ba"
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDkPvNtpy-ba"
   },
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # These attributes are defined here, but since they are\n",
    "        # not available at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "\n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader \n",
    "        # (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to create a SummaryWriter to \n",
    "        # interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(\n",
    "            folder, name, suffix\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4TFCMchy-ba"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRafKRCFy-bb"
   },
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # These attributes are defined here, but since they are\n",
    "        # not available at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "\n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader \n",
    "        # (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to create a SummaryWriter to \n",
    "        # interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(\n",
    "            folder, name, suffix\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o6rauUmy-bb"
   },
   "source": [
    "### Step Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niHfS5_Ky-bb"
   },
   "outputs": [],
   "source": [
    "def _make_train_step(self):\n",
    "    # This method does not need ARGS... it can refer to\n",
    "    # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def perform_train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = self.model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "        loss.backward()\n",
    "        # Step 4 - Updates parameters using gradients and the\n",
    "        # learning rate\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return perform_train_step\n",
    "\n",
    "def _make_val_step(self):\n",
    "    # Builds function that performs a step in the validation loop\n",
    "    def perform_val_step(x, y):\n",
    "        # Sets model to EVAL mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Step 1 - Computes our model's predicted output - forward pass\n",
    "        yhat = self.model(x)\n",
    "        # Step 2 - Computes the loss\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        # There is no need to compute Steps 3 and 4, \n",
    "        # since we don't update parameters during evaluation\n",
    "        return loss.item()\n",
    "\n",
    "    return perform_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FVF0L3sy-bc"
   },
   "outputs": [],
   "source": [
    "# ATTENTION! Using SETATTR for educational purposes only :-)\n",
    "setattr(StepByStep, '_make_train_step', _make_train_step)\n",
    "setattr(StepByStep, '_make_val_step', _make_val_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlx8CJuIy-bc"
   },
   "source": [
    "### setattr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvnbqvwMy-bc"
   },
   "outputs": [],
   "source": [
    "class Dog(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3yvS1JGy-bd"
   },
   "outputs": [],
   "source": [
    "rex = Dog('Rex')\n",
    "print(rex.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkj7_VaYy-bd"
   },
   "outputs": [],
   "source": [
    "def bark(dog):\n",
    "    print('{} barks: \"Woof!\"'.format(dog.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UP2a37Z-y-be"
   },
   "outputs": [],
   "source": [
    "bark(rex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIw_XBxTy-be"
   },
   "outputs": [],
   "source": [
    "def bark(self):\n",
    "    print('{} barks: \"Woof!\"'.format(self.name))\n",
    "\n",
    "setattr(Dog, 'bark', bark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtH0aaD5y-be"
   },
   "outputs": [],
   "source": [
    "fido = Dog('Fido')\n",
    "fido.bark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hkwny6jzy-be"
   },
   "outputs": [],
   "source": [
    "rex.bark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RihXn1djy-bf"
   },
   "source": [
    "## Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNW8qilEy-bf"
   },
   "source": [
    "### Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyG9XV9wy-bf"
   },
   "outputs": [],
   "source": [
    "def _mini_batch(self, validation=False):\n",
    "    # The mini-batch can be used with both loaders\n",
    "    # The argument `validation`defines which loader and \n",
    "    # corresponding step function is going to be used\n",
    "    if validation:\n",
    "        data_loader = self.val_loader\n",
    "        step = self.val_step\n",
    "    else:\n",
    "        data_loader = self.train_loader\n",
    "        step = self.train_step\n",
    "\n",
    "    if data_loader is None:\n",
    "        return None\n",
    "\n",
    "    # Once the data loader and step function, this is the same\n",
    "    # mini-batch loop we had before\n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch = x_batch.to(self.device)\n",
    "        y_batch = y_batch.to(self.device)\n",
    "\n",
    "        mini_batch_loss = step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "\n",
    "    return loss\n",
    "\n",
    "setattr(StepByStep, '_mini_batch', _mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgkvCrdvy-bf"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1MHno-cy-bf"
   },
   "outputs": [],
   "source": [
    "def set_seed(self, seed=42):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "setattr(StepByStep, 'set_seed', set_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5uDK5E-y-bf"
   },
   "outputs": [],
   "source": [
    "def train(self, n_epochs, seed=42):\n",
    "    # To ensure reproducibility of the training process\n",
    "    self.set_seed(seed)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Keeps track of the numbers of epochs\n",
    "        # by updating the corresponding attribute\n",
    "        self.total_epochs += 1\n",
    "\n",
    "        # inner loop\n",
    "        # Performs training using mini-batches\n",
    "        loss = self._mini_batch(validation=False)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # VALIDATION\n",
    "        # no gradients in validation!\n",
    "        with torch.no_grad():\n",
    "            # Performs evaluation using mini-batches\n",
    "            val_loss = self._mini_batch(validation=True)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "        # If a SummaryWriter has been set...\n",
    "        if self.writer:\n",
    "            scalars = {'training': loss}\n",
    "            if val_loss is not None:\n",
    "                scalars.update({'validation': val_loss})\n",
    "            # Records both losses for each epoch under the main tag \"loss\"\n",
    "            self.writer.add_scalars(main_tag='loss',\n",
    "                                    tag_scalar_dict=scalars,\n",
    "                                    global_step=epoch)\n",
    "\n",
    "    if self.writer:\n",
    "        # Flushes the writer\n",
    "        self.writer.flush()\n",
    "        \n",
    "setattr(StepByStep, 'train', train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx8xascBy-bg"
   },
   "source": [
    "## Saving and Loading Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyTn7Xnmy-bg"
   },
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8seA6Hmy-bg"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(self, filename):\n",
    "    # Builds dictionary with all elements for resuming training\n",
    "    checkpoint = {'epoch': self.total_epochs,\n",
    "                  'model_state_dict': self.model.state_dict(),\n",
    "                  'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                  'loss': self.losses,\n",
    "                  'val_loss': self.val_losses}\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "setattr(StepByStep, 'save_checkpoint', save_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGnfunNRy-bg"
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb_XAxvEy-bg"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(self, filename):\n",
    "    # Loads dictionary\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    # Restore state for model and optimizer\n",
    "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    self.total_epochs = checkpoint['epoch']\n",
    "    self.losses = checkpoint['loss']\n",
    "    self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "    self.model.train() # always use TRAIN for resuming training   \n",
    "    \n",
    "setattr(StepByStep, 'load_checkpoint', load_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKueWti2y-bg"
   },
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JHhhaiBy-bg"
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Set is to evaluation mode for predictions\n",
    "    self.model.eval() \n",
    "    # Takes aNumpy input and make it a float tensor\n",
    "    x_tensor = torch.as_tensor(x).float()\n",
    "    # Send input to device and uses model for prediction\n",
    "    y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "    # Set it back to train mode\n",
    "    self.model.train()\n",
    "    # Detaches it, brings it to CPU and back to Numpy\n",
    "    return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "setattr(StepByStep, 'predict', predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pU5o1059y-bh"
   },
   "source": [
    "## Visualization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcCWJLedy-bh"
   },
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfR8yKYty-bh"
   },
   "outputs": [],
   "source": [
    "def plot_losses(self):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(self.losses, label='Training Loss', c='b')\n",
    "    if self.val_loader:\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "setattr(StepByStep, 'plot_losses', plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I6ljbAby-bh"
   },
   "source": [
    "### Model Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkEy4mQ-y-bh"
   },
   "outputs": [],
   "source": [
    "def add_graph(self):\n",
    "    if self.train_loader and self.writer:\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        x_sample, y_sample = next(iter(self.train_loader))\n",
    "        self.writer.add_graph(self.model, x_sample.to(self.device))\n",
    "    \n",
    "setattr(StepByStep, 'add_graph', add_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG6FI3AVy-bh"
   },
   "source": [
    "## The Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaenAWo4y-bh"
   },
   "outputs": [],
   "source": [
    "# %load stepbystep/v0.py\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(\n",
    "            folder, name, suffix\n",
    "        ))\n",
    "\n",
    "    def _make_train_step(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, \n",
    "            # since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the \n",
    "        # same mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TOWxdm3y-bi"
   },
   "source": [
    "## Classy Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhwa_Dmfy-bi"
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6La0FB_ny-bi"
   },
   "outputs": [],
   "source": [
    "# Runs data generation - so we do not need to copy code here\n",
    "%run -i data_generation/simple_linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGmyXE7vy-bi"
   },
   "outputs": [],
   "source": [
    "fig = figure1(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh-AeQfey-bj"
   },
   "source": [
    "### Data Preparation V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkvsO4v3y-bj"
   },
   "outputs": [],
   "source": [
    "# %load data_preparation/v2.py\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.as_tensor(x).float()\n",
    "y_tensor = torch.as_tensor(y).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Performs the split\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebnmjMsxy-bj"
   },
   "source": [
    "### Model Configuration V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMGNhJmHy-bj"
   },
   "outputs": [],
   "source": [
    "%%writefile model_configuration/v4.py\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(1, 1))\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV0W5nHCy-bj"
   },
   "outputs": [],
   "source": [
    "%run -i model_configuration/v4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q--ioJGay-bj"
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuGpFf9Hy-bk"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZGeZtESy-bk"
   },
   "source": [
    "### Cell 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZxO-yDQy-bk"
   },
   "outputs": [],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.set_tensorboard('classy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp2SzWNBy-bk"
   },
   "outputs": [],
   "source": [
    "print(sbs.model == model)\n",
    "print(sbs.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oflcv_3Qy-bk"
   },
   "source": [
    "### Cell 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_5P_4CXy-bk"
   },
   "outputs": [],
   "source": [
    "sbs.train(n_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VR7jHvWky-bk"
   },
   "outputs": [],
   "source": [
    "print(model.state_dict()) # remember, model == sbs.model\n",
    "print(sbs.total_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAJGX6uGy-bl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G0pptoUy-bl"
   },
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w85aI3cjy-bl"
   },
   "outputs": [],
   "source": [
    "new_data = np.array([.5, .3, .7]).reshape(-1, 1)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuByoK7Zy-bl"
   },
   "outputs": [],
   "source": [
    "predictions = sbs.predict(new_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJ2gBxPKy-bl"
   },
   "source": [
    "### Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HatDDEWPy-bm"
   },
   "source": [
    "### Cell 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_sroAZcy-bm"
   },
   "outputs": [],
   "source": [
    "sbs.save_checkpoint('model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzusvqrly-bm"
   },
   "source": [
    "### Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh3rbXQMy-bm"
   },
   "outputs": [],
   "source": [
    "%run -i model_configuration/v4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRGLESXny-bm"
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3tJSHkMy-bm"
   },
   "source": [
    "### Cell 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfeJme9iy-bm"
   },
   "outputs": [],
   "source": [
    "new_sbs = StepByStep(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGugDXOJy-bn"
   },
   "source": [
    "### Cell 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK5-KY6wy-bn"
   },
   "outputs": [],
   "source": [
    "new_sbs.load_checkpoint('model_checkpoint.pth')\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kUDWsIuy-bn"
   },
   "source": [
    "### Cell 5.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK9w53xQy-bn"
   },
   "outputs": [],
   "source": [
    "new_sbs.set_loaders(train_loader, val_loader)\n",
    "new_sbs.train(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaDQj1zqy-bn"
   },
   "outputs": [],
   "source": [
    "fig = new_sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79ZfRtchy-bn"
   },
   "outputs": [],
   "source": [
    "print(sbs.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qydws-P7y-bo"
   },
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enrjpfVXy-bo"
   },
   "outputs": [],
   "source": [
    "# %load data_preparation/v2.py\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.as_tensor(x).float()\n",
    "y_tensor = torch.as_tensor(y).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Performs the split\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * ratio)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_XFz52Dy-bo"
   },
   "outputs": [],
   "source": [
    "# %load model_configuration/v4.py\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model\n",
    "model = nn.Sequential(nn.Linear(1, 1))\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters \n",
    "# (now retrieved directly from the model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My3Ak3Gsy-bo"
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.set_tensorboard('classy')\n",
    "sbs.train(n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwn4wcACy-br"
   },
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOChTrjxy-br"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DdlplLyKy-bW",
    "qnVChfy0y-bY",
    "0o6rauUmy-bb",
    "zlx8CJuIy-bc",
    "UNW8qilEy-bf",
    "pgkvCrdvy-bf",
    "OyTn7Xnmy-bg",
    "SGnfunNRy-bg",
    "TKueWti2y-bg",
    "tcCWJLedy-bh",
    "0I6ljbAby-bh",
    "Fhwa_Dmfy-bi",
    "nh-AeQfey-bj",
    "ebnmjMsxy-bj",
    "6ZGeZtESy-bk",
    "oflcv_3Qy-bk",
    "5G0pptoUy-bl",
    "yJ2gBxPKy-bl",
    "HatDDEWPy-bm",
    "wzusvqrly-bm",
    "Q3tJSHkMy-bm",
    "gGugDXOJy-bn",
    "5kUDWsIuy-bn",
    "Qydws-P7y-bo"
   ],
   "name": "Chapter05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
