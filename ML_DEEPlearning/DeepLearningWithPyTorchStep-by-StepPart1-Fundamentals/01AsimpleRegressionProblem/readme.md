* Briefly review the steps of gradient descent (optional).

* Use gradient descent to implement a linear regression in Numpy.

* Create tensors in PyTorch (finally!).

* Understand the difference between CPU and GPU tensors.

* Understand PyTorch’s main feature, autograd, to perform automatic differentiation.

* Visualize the dynamic computation graph.

* Create a loss function.

* Define an optimizer.

* Implement our own model class.

* Implement nested and sequential models using PyTorch’s layers.

## Types of layers

There are many different layers that can be used in PyTorch; a few of which have been mentioned below:

* Convolution layers

* Pooling layers

* Padding layers

* Non-linear activations

* Normalization layers

* Recurrent layers

* Transformer layers

* Linear layers

* Dropout layers

* Sparse layers (embeddings)

* Vision layers

* DataParallel layers (multi-GPU)

* Flatten layer
